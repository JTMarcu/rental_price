{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92c6b3b",
   "metadata": {},
   "source": [
    "This script is a web scraper designed to collect rental property data from the website **apartments.com** for properties in San Diego County, California, under $4,000. It uses **Selenium** for browser automation and **BeautifulSoup** for HTML parsing. The script processes the data, cleans it, and saves it to a CSV file. Here's a breakdown of its functionality:\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Components**\n",
    "\n",
    "1. **Imports and Configuration**\n",
    "   - Libraries like `selenium`, `BeautifulSoup`, `pandas`, `logging`, and `re` are imported.\n",
    "   - Constants are defined:\n",
    "     - `HEADLESS`: Runs the browser in headless mode (no GUI).\n",
    "     - `WAIT_TIME`: Time to wait for pages to load.\n",
    "     - `LISTINGS_PER_PAGE`: Number of listings per page.\n",
    "     - `BASE_URL`: The URL for the search query.\n",
    "     - `LOG_FILE`: File for logging warnings and errors.\n",
    "     - `TEST_MODE` and `MAX_UNITS`: Used for testing to limit the number of listings scraped.\n",
    "\n",
    "2. **`init_driver()`**\n",
    "   - Initializes a Selenium WebDriver for Microsoft Edge.\n",
    "   - Configures the browser to run in headless mode and sets a custom user-agent.\n",
    "\n",
    "3. **`extract_low_price(price)`**\n",
    "   - Extracts the lowest price from a price string (e.g., \"$1,200-$1,500\" → `1200`).\n",
    "\n",
    "4. **`extract_amenities(soup)`**\n",
    "   - Parses the HTML of a property page to extract amenities like washer/dryer, air conditioning, pool, gym, etc.\n",
    "   - Returns a dictionary of boolean values indicating the presence of each amenity.\n",
    "\n",
    "5. **`scrape_listings(driver)`**\n",
    "   - The main scraping function:\n",
    "     - Iterates through pages of listings.\n",
    "     - For each listing, navigates to the property detail page and extracts data such as:\n",
    "       - Property name, address, unit details, price, square footage, beds, baths, rental type, phone number, and amenities.\n",
    "     - Appends the data to a list of dictionaries (`all_units`).\n",
    "     - Stops scraping if `TEST_MODE` is enabled and the maximum number of units is reached.\n",
    "   - Returns a Pandas DataFrame containing all the scraped data.\n",
    "\n",
    "6. **`clean_data(df)`**\n",
    "   - Cleans and processes the scraped data:\n",
    "     - Converts price, square footage, beds, and baths to numeric values.\n",
    "     - Extracts city, state, and ZIP code from the address.\n",
    "     - Calculates price per square foot.\n",
    "     - Creates a \"Beds_Baths\" column summarizing the number of beds and baths.\n",
    "     - Reorders columns into a final format for saving.\n",
    "\n",
    "7. **`main()`**\n",
    "   - The entry point of the script:\n",
    "     - Initializes the WebDriver.\n",
    "     - Calls `scrape_listings()` to collect data.\n",
    "     - Cleans the data using `clean_data()`.\n",
    "     - Saves the cleaned data to a CSV file named with the current date.\n",
    "     - Logs the runtime of the script.\n",
    "\n",
    "---\n",
    "\n",
    "### **Workflow**\n",
    "\n",
    "1. **Initialization**\n",
    "   - The script sets up the WebDriver and logging.\n",
    "\n",
    "2. **Scraping**\n",
    "   - It navigates through pages of listings on **apartments.com**.\n",
    "   - For each listing, it extracts relevant details and amenities.\n",
    "\n",
    "3. **Data Cleaning**\n",
    "   - The raw data is processed to ensure consistency and usability.\n",
    "\n",
    "4. **Saving Results**\n",
    "   - The cleaned data is saved to a CSV file in the format: `san_diego_county_rentals_YYYY-MM-DD.csv`.\n",
    "\n",
    "5. **Runtime Logging**\n",
    "   - The script logs warnings, errors, and runtime information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Features**\n",
    "- **Headless Mode**: Allows the script to run without opening a browser window.\n",
    "- **Error Handling**: Logs warnings for errors encountered during scraping.\n",
    "- **Test Mode**: Limits the number of listings scraped for testing purposes.\n",
    "- **Data Cleaning**: Ensures the final dataset is well-structured and ready for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### **Output**\n",
    "The script generates a CSV file containing the following columns:\n",
    "- Property details (name, address, city, state, ZIP code, phone number, etc.).\n",
    "- Unit details (price, square footage, beds, baths, rental type).\n",
    "- Amenities (washer/dryer, air conditioning, pool, gym, etc.).\n",
    "- Calculated fields (price per square foot, beds/baths summary).\n",
    "\n",
    "---\n",
    "\n",
    "### **Usage**\n",
    "Run the script by executing it in the terminal:\n",
    "```bash\n",
    "python scraper.py\n",
    "```\n",
    "Ensure that the required dependencies (e.g., Selenium, BeautifulSoup, Pandas) are installed, and the WebDriver for Edge is properly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aab9b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9daf75",
   "metadata": {},
   "source": [
    "This code imports various libraries and modules that are essential for the web scraping script. Here's a breakdown of each import:\n",
    "\n",
    "---\n",
    "\n",
    "### **Selenium Imports**\n",
    "1. **`from selenium import webdriver`**:\n",
    "    - Imports the `webdriver` module from Selenium, which is used to automate browser interactions.\n",
    "\n",
    "2. **`from selenium.webdriver.edge.service import Service`**:\n",
    "    - Imports the `Service` class, which is used to manage the Microsoft Edge WebDriver.\n",
    "\n",
    "3. **`from selenium.webdriver.edge.options import Options`**:\n",
    "    - Imports the `Options` class, which allows you to configure browser settings (e.g., headless mode, user-agent).\n",
    "\n",
    "4. **`from webdriver_manager.microsoft import EdgeChromiumDriverManager`**:\n",
    "    - Imports the `EdgeChromiumDriverManager` class, which automatically downloads and manages the correct version of the Microsoft Edge WebDriver.\n",
    "\n",
    "---\n",
    "\n",
    "### **BeautifulSoup Import**\n",
    "5. **`from bs4 import BeautifulSoup`**:\n",
    "    - Imports the `BeautifulSoup` class from the `bs4` library, which is used for parsing and navigating HTML content.\n",
    "\n",
    "---\n",
    "\n",
    "### **Pandas Import**\n",
    "6. **`import pandas as pd`**:\n",
    "    - Imports the `pandas` library, which is used for data manipulation and analysis. It is commonly used to work with tabular data in the form of DataFrames.\n",
    "\n",
    "---\n",
    "\n",
    "### **Time and OS Imports**\n",
    "7. **`import time`**:\n",
    "    - Imports the `time` module, which provides functions for working with time (e.g., delays using `time.sleep()`).\n",
    "\n",
    "8. **`import os`**:\n",
    "    - Imports the `os` module, which provides functions for interacting with the operating system (e.g., file paths, environment variables).\n",
    "\n",
    "---\n",
    "\n",
    "### **Logging Import**\n",
    "9. **`import logging`**:\n",
    "    - Imports the `logging` module, which is used for logging messages (e.g., warnings, errors, runtime information).\n",
    "\n",
    "---\n",
    "\n",
    "### **Regex Import**\n",
    "10. **`import re`**:\n",
    "     - Imports the `re` module, which provides functions for working with regular expressions (e.g., pattern matching, text extraction).\n",
    "\n",
    "---\n",
    "\n",
    "### **Datetime Import**\n",
    "11. **`from datetime import datetime`**:\n",
    "     - Imports the `datetime` class from the `datetime` module, which is used for working with dates and times (e.g., generating timestamps).\n",
    "\n",
    "---\n",
    "\n",
    "### **Purpose**\n",
    "These imports collectively enable the script to:\n",
    "1. Automate browser interactions using Selenium.\n",
    "2. Parse and extract data from HTML using BeautifulSoup.\n",
    "3. Manipulate and analyze data using Pandas.\n",
    "4. Handle time delays, file paths, and logging.\n",
    "5. Use regular expressions for text processing.\n",
    "6. Work with dates and times for logging and file naming.\n",
    "\n",
    "These libraries and modules are essential for building a robust and efficient web scraping script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3f96b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADLESS = True\n",
    "WAIT_TIME = 4\n",
    "LISTINGS_PER_PAGE = 40\n",
    "BASE_URL = \"https://www.apartments.com/apartments-condos/san-diego-county-ca/under-4000/\"\n",
    "LOG_FILE = \"scraper_log.txt\"\n",
    "TEST_MODE = True\n",
    "MAX_UNITS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b565e2e",
   "metadata": {},
   "source": [
    "### Explanation of Variables\n",
    "\n",
    "1. **`HEADLESS = True`**:\n",
    "    - This variable determines whether the browser runs in headless mode. \n",
    "    - When set to `True`, the browser operates without a graphical user interface (GUI), making it faster and suitable for automated tasks.\n",
    "\n",
    "2. **`WAIT_TIME = 4`**:\n",
    "    - Specifies the number of seconds to wait for a page to load or for elements to appear during web scraping.\n",
    "    - This helps ensure that the script doesn't proceed before the page is fully loaded.\n",
    "\n",
    "3. **`LISTINGS_PER_PAGE = 40`**:\n",
    "    - Indicates the number of property listings displayed per page on the website being scraped.\n",
    "    - This value is used to determine when to stop scraping if fewer listings are found on a page.\n",
    "\n",
    "4. **`BASE_URL = \"https://www.apartments.com/apartments-condos/san-diego-county-ca/under-4000/\"`**:\n",
    "    - The base URL of the website to scrape. \n",
    "    - It points to rental property listings in San Diego County, California, with a price cap of $4,000.\n",
    "\n",
    "5. **`LOG_FILE = \"scraper_log.txt\"`**:\n",
    "    - The name of the file where warnings, errors, and runtime information are logged.\n",
    "    - Useful for debugging and tracking the script's execution.\n",
    "\n",
    "6. **`TEST_MODE = True`**:\n",
    "    - When set to `True`, the script runs in test mode, limiting the number of listings scraped.\n",
    "    - This is helpful for debugging or testing the script without scraping the entire dataset.\n",
    "\n",
    "7. **`MAX_UNITS = 10`**:\n",
    "    - Specifies the maximum number of property listings to scrape when `TEST_MODE` is enabled.\n",
    "    - Prevents excessive data collection during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2161d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    filemode='w',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.WARNING\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e3585",
   "metadata": {},
   "source": [
    "### Explanation of `logging.basicConfig`\n",
    "\n",
    "The `logging.basicConfig` function is used to configure the logging settings for the script. Here's a breakdown of each parameter:\n",
    "\n",
    "1. **`filename=LOG_FILE`**:\n",
    "    - Specifies the name of the file where log messages will be saved.\n",
    "    - In this case, the value of `LOG_FILE` is `'scraper_log.txt'`, so all log messages will be written to this file.\n",
    "\n",
    "2. **`filemode='w'`**:\n",
    "    - Sets the mode for opening the log file.\n",
    "    - `'w'` means the file will be overwritten each time the script runs. If you want to append to the file instead, use `'a'`.\n",
    "\n",
    "3. **`format='%(asctime)s - %(levelname)s - %(message)s'`**:\n",
    "    - Defines the format of log messages.\n",
    "    - `%(asctime)s`: Includes the timestamp of when the log message was created.\n",
    "    - `%(levelname)s`: Includes the severity level of the log message (e.g., WARNING, ERROR).\n",
    "    - `%(message)s`: Includes the actual log message.\n",
    "\n",
    "4. **`level=logging.WARNING`**:\n",
    "    - Sets the minimum severity level of messages to be logged.\n",
    "    - In this case, only messages with a severity of WARNING or higher (e.g., ERROR, CRITICAL) will be logged. Lower severity levels like DEBUG or INFO will be ignored.\n",
    "\n",
    "### Example Log Entry\n",
    "A log entry in the file might look like this:\n",
    "```\n",
    "2023-03-15 14:30:45,123 - WARNING - This is a warning message.\n",
    "```\n",
    "\n",
    "This configuration ensures that important warnings and errors are recorded in the specified log file for debugging and monitoring purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc77cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_driver():\n",
    "    options = Options()\n",
    "    if HEADLESS:\n",
    "        options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "    service = Service(EdgeChromiumDriverManager().install(), log_output=os.devnull)\n",
    "    return webdriver.Edge(service=service, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecad7e9",
   "metadata": {},
   "source": [
    "### Explanation of `init_driver` Function\n",
    "\n",
    "The `init_driver` function initializes and configures a Selenium WebDriver for Microsoft Edge. Here's a detailed breakdown of its components:\n",
    "\n",
    "1. **`options = Options()`**:\n",
    "    - Creates an instance of the `Options` class to configure browser settings.\n",
    "\n",
    "2. **`if HEADLESS:`**:\n",
    "    - Checks if the `HEADLESS` variable is set to `True`.\n",
    "    - If `True`, the browser will run in headless mode (without a graphical user interface), which is faster and suitable for automated tasks.\n",
    "\n",
    "3. **`options.add_argument(\"--headless\")`**:\n",
    "    - Adds the `--headless` argument to enable headless mode.\n",
    "\n",
    "4. **`options.add_argument(\"--disable-gpu\")`**:\n",
    "    - Disables GPU hardware acceleration. This is often used in headless mode to avoid potential rendering issues.\n",
    "\n",
    "5. **`options.add_argument(\"--no-sandbox\")`**:\n",
    "    - Disables the sandboxing feature, which is sometimes required for running the browser in certain environments (e.g., Docker containers).\n",
    "\n",
    "6. **`options.add_argument(\"user-agent=Mozilla/5.0\")`**:\n",
    "    - Sets a custom user-agent string to mimic a real browser and avoid detection by websites.\n",
    "\n",
    "7. **`service = Service(EdgeChromiumDriverManager().install(), log_output=os.devnull)`**:\n",
    "    - Creates a `Service` instance for managing the Edge WebDriver.\n",
    "    - `EdgeChromiumDriverManager().install()` automatically downloads and installs the appropriate version of the Edge WebDriver.\n",
    "    - `log_output=os.devnull` suppresses WebDriver logs by redirecting them to `/dev/null`.\n",
    "\n",
    "8. **`return webdriver.Edge(service=service, options=options)`**:\n",
    "    - Initializes and returns an instance of the Edge WebDriver with the specified `service` and `options`.\n",
    "\n",
    "### Purpose\n",
    "This function sets up a Selenium WebDriver with custom configurations, such as headless mode and user-agent spoofing, to automate browser interactions efficiently and avoid detection during web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c6c8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_low_price(price):\n",
    "    if pd.isna(price):\n",
    "        return None\n",
    "    price = re.sub(r'[^\\d\\-]', '', str(price))\n",
    "    return float(price.split('-')[0]) if '-' in price else float(price) if price.isdigit() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec82c7",
   "metadata": {},
   "source": [
    "### Explanation of `extract_low_price` Function\n",
    "\n",
    "The `extract_low_price` function is designed to extract the lowest price from a given price string. Here's a detailed breakdown of its logic:\n",
    "\n",
    "1. **`if pd.isna(price):`**\n",
    "    - Checks if the input `price` is `NaN` (Not a Number) or missing using Pandas' `isna()` function.\n",
    "    - If the price is missing, the function returns `None`.\n",
    "\n",
    "2. **`price = re.sub(r'[^\\d\\-]', '', str(price))`**\n",
    "    - Converts the `price` to a string (if it isn't already).\n",
    "    - Uses the `re.sub()` function to remove all characters except digits (`\\d`) and hyphens (`-`).\n",
    "    - This ensures that only numeric values and ranges (e.g., \"1200-1500\") are retained.\n",
    "\n",
    "3. **`return float(price.split('-')[0]) if '-' in price else float(price) if price.isdigit() else None`**\n",
    "    - Checks if the cleaned `price` contains a hyphen (`-`):\n",
    "        - If `True`, splits the string at the hyphen and takes the first part (the lower bound of the range), converting it to a float.\n",
    "    - If there is no hyphen, checks if the `price` is a valid numeric string using `isdigit()`:\n",
    "        - If `True`, converts the string to a float and returns it.\n",
    "    - If neither condition is met (e.g., the string is empty or invalid), the function returns `None`.\n",
    "\n",
    "### Purpose\n",
    "This function is used to standardize and extract the lowest price from various price formats, such as:\n",
    "- \"$1,200-$1,500\" → `1200`\n",
    "- \"$1,200\" → `1200`\n",
    "- Invalid or missing prices → `None`\n",
    "\n",
    "### Example Usage\n",
    "```python\n",
    "extract_low_price(\"$1,200-$1,500\")  # Output: 1200.0\n",
    "extract_low_price(\"$1,200\")         # Output: 1200.0\n",
    "extract_low_price(\"N/A\")            # Output: None\n",
    "extract_low_price(None)             # Output: None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01484a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_amenities(soup):\n",
    "    labels = soup.select('.amenityLabel') + soup.select('.combinedAmenitiesList li span')\n",
    "    text = ' '.join(el.get_text(separator=' ').lower().strip() for el in labels)\n",
    "\n",
    "    logging.debug(\"Combined amenities text: %s\", text)\n",
    "\n",
    "    return {\n",
    "        'HasWasherDryer': 'washer/dryer' in text or 'in unit washer' in text,\n",
    "        'HasAirConditioning': 'air conditioning' in text,\n",
    "        'HasPool': 'pool' in text,\n",
    "        'HasSpa': 'spa' in text or 'hot tub' in text,\n",
    "        'HasGym': 'fitness center' in text or 'gym' in text,\n",
    "        'HasEVCharging': 'ev charging' in text,\n",
    "        'AllowsDogs': 'dogs allowed' in text or 'dog friendly' in text,\n",
    "        'AllowsCats': 'cats allowed' in text or 'cat friendly' in text\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27530f1",
   "metadata": {},
   "source": [
    "### Explanation of `extract_amenities` Function\n",
    "\n",
    "The `extract_amenities` function is designed to parse the HTML content of a property listing page and extract information about the amenities available. Here's a detailed breakdown of its components:\n",
    "\n",
    "1. **`labels = soup.select('.amenityLabel') + soup.select('.combinedAmenitiesList li span')`**:\n",
    "    - Uses BeautifulSoup's `select()` method to find all elements matching the specified CSS selectors:\n",
    "        - `.amenityLabel`: Selects elements with the class `amenityLabel`.\n",
    "        - `.combinedAmenitiesList li span`: Selects `<span>` elements inside `<li>` elements within the `combinedAmenitiesList` class.\n",
    "    - Combines the results from both selectors into a single list of elements.\n",
    "\n",
    "2. **`text = ' '.join(el.get_text(separator=' ').lower().strip() for el in labels)`**:\n",
    "    - Iterates over each element in the `labels` list.\n",
    "    - Extracts the text content of each element using `get_text(separator=' ')`.\n",
    "    - Converts the text to lowercase using `.lower()` for case-insensitive matching.\n",
    "    - Strips any leading or trailing whitespace using `.strip()`.\n",
    "    - Joins all the extracted text into a single string, separated by spaces.\n",
    "\n",
    "3. **`logging.debug(\"Combined amenities text: %s\", text)`**:\n",
    "    - Logs the combined amenities text at the DEBUG level for debugging purposes.\n",
    "    - This helps in verifying the extracted text during development or troubleshooting.\n",
    "\n",
    "4. **`return { ... }`**:\n",
    "    - Returns a dictionary where each key represents a specific amenity, and the value is a boolean indicating whether the amenity is present in the `text`.\n",
    "    - The presence of each amenity is determined by checking if specific keywords or phrases are found in the `text`:\n",
    "        - `'HasWasherDryer'`: Checks for `'washer/dryer'` or `'in unit washer'`.\n",
    "        - `'HasAirConditioning'`: Checks for `'air conditioning'`.\n",
    "        - `'HasPool'`: Checks for `'pool'`.\n",
    "        - `'HasSpa'`: Checks for `'spa'` or `'hot tub'`.\n",
    "        - `'HasGym'`: Checks for `'fitness center'` or `'gym'`.\n",
    "        - `'HasEVCharging'`: Checks for `'ev charging'`.\n",
    "        - `'AllowsDogs'`: Checks for `'dogs allowed'` or `'dog friendly'`.\n",
    "        - `'AllowsCats'`: Checks for `'cats allowed'` or `'cat friendly'`.\n",
    "\n",
    "### Purpose\n",
    "This function extracts and standardizes information about property amenities from the HTML content of a listing page. The resulting dictionary can be used to store or analyze the availability of specific amenities for each property.\n",
    "\n",
    "### Example Usage\n",
    "```python\n",
    "# Assuming `soup` is a BeautifulSoup object containing the HTML of a property page\n",
    "amenities = extract_amenities(soup)\n",
    "print(amenities)\n",
    "# Output:\n",
    "# {\n",
    "#     'HasWasherDryer': True,\n",
    "#     'HasAirConditioning': False,\n",
    "#     'HasPool': True,\n",
    "#     'HasSpa': False,\n",
    "#     'HasGym': True,\n",
    "#     'HasEVCharging': False,\n",
    "#     'AllowsDogs': True,\n",
    "#     'AllowsCats': False\n",
    "# }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cfa8328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_listings(driver):\n",
    "    all_units = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        url = f\"{BASE_URL}{page}/\"\n",
    "        logging.info(f\"Scraping page {page}: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(WAIT_TIME)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        listings = soup.find_all('article')\n",
    "\n",
    "        if not listings:\n",
    "            break\n",
    "\n",
    "        for listing in listings:\n",
    "            title = listing.find('span', class_='js-placardTitle')\n",
    "            address = listing.find('div', class_='property-address')\n",
    "            phone = listing.find('button', class_='phone-link')\n",
    "            property_url = listing.get('data-url')\n",
    "\n",
    "            if title and address and property_url:\n",
    "                try:\n",
    "                    driver.get(property_url)\n",
    "                    time.sleep(WAIT_TIME / 2)\n",
    "                    detail_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    unit_containers = detail_soup.find_all('li', class_='unitContainer js-unitContainerV3')\n",
    "\n",
    "                    rental_type = \"Unknown\"\n",
    "                    og_title_tag = detail_soup.find(\"meta\", property=\"og:title\")\n",
    "                    if og_title_tag and og_title_tag.get(\"content\"):\n",
    "                        content = og_title_tag[\"content\"].lower()\n",
    "                        for term in [\"house rental\", \"townhome\", \"condo\", \"apartment\"]:\n",
    "                            if term in content:\n",
    "                                rental_type = term.replace(\" rental\", \"\").capitalize()\n",
    "\n",
    "                    amenities = extract_amenities(detail_soup)\n",
    "\n",
    "                    for unit in unit_containers:\n",
    "                        unit_number = unit.find('div', class_='unitColumn column')\n",
    "                        price = unit.find('div', class_='pricingColumn column')\n",
    "                        sqft = unit.find('div', class_='sqftColumn column')\n",
    "                        beds = unit.get('data-beds')\n",
    "                        baths = unit.get('data-baths')\n",
    "\n",
    "                        all_units.append({\n",
    "                            'Property': title.text.strip(),\n",
    "                            'Address': address.text.strip(),\n",
    "                            'Unit': unit_number.text.strip() if unit_number else \"N/A\",\n",
    "                            'Price': price.text.strip() if price else \"N/A\",\n",
    "                            'SqFt': sqft.text.strip() if sqft else \"N/A\",\n",
    "                            'Beds': beds if beds else \"N/A\",\n",
    "                            'Baths': baths if baths else \"N/A\",\n",
    "                            'RentalType': rental_type,\n",
    "                            'Phone': phone.get('phone-data') if phone and phone.has_attr('phone-data') else \"N/A\",\n",
    "                            **amenities,\n",
    "                            'StorageFee': None,\n",
    "                            'ListingURL': property_url\n",
    "                        })\n",
    "\n",
    "                        if TEST_MODE and len(all_units) >= MAX_UNITS:\n",
    "                            logging.info(f\"TEST_MODE: Stopping after {MAX_UNITS} listings.\")\n",
    "                            return pd.DataFrame(all_units)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error processing {property_url}: {e}\")\n",
    "\n",
    "        if len(listings) < LISTINGS_PER_PAGE:\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "    return pd.DataFrame(all_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfccb19",
   "metadata": {},
   "source": [
    "### Explanation of `scrape_listings` Function\n",
    "\n",
    "The `scrape_listings` function is the core of the web scraping process. It navigates through multiple pages of property listings, extracts relevant details, and compiles them into a structured dataset. Here's a detailed breakdown:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Initialization**\n",
    "- **`all_units = []`**:\n",
    "    - Initializes an empty list to store data for all property units.\n",
    "- **`page = 1`**:\n",
    "    - Starts scraping from the first page of listings.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Page Navigation**\n",
    "- **`while True:`**:\n",
    "    - Enters an infinite loop to scrape pages until no more listings are found.\n",
    "- **`url = f\"{BASE_URL}{page}/\"`**:\n",
    "    - Constructs the URL for the current page using the `BASE_URL` and `page` number.\n",
    "- **`logging.info(f\"Scraping page {page}: {url}\")`**:\n",
    "    - Logs the current page being scraped.\n",
    "- **`driver.get(url)`**:\n",
    "    - Navigates to the constructed URL using the Selenium WebDriver.\n",
    "- **`time.sleep(WAIT_TIME)`**:\n",
    "    - Waits for the page to load completely before proceeding.\n",
    "- **`soup = BeautifulSoup(driver.page_source, 'html.parser')`**:\n",
    "    - Parses the page's HTML content using BeautifulSoup.\n",
    "- **`listings = soup.find_all('article')`**:\n",
    "    - Finds all property listings on the page, which are contained in `<article>` elements.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Exit Condition**\n",
    "- **`if not listings:`**:\n",
    "    - If no listings are found, the loop breaks, indicating the end of available pages.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Listing Details Extraction**\n",
    "- Iterates through each listing on the page:\n",
    "    - **`title = listing.find('span', class_='js-placardTitle')`**:\n",
    "        - Extracts the property title.\n",
    "    - **`address = listing.find('div', class_='property-address')`**:\n",
    "        - Extracts the property address.\n",
    "    - **`phone = listing.find('button', class_='phone-link')`**:\n",
    "        - Extracts the phone number.\n",
    "    - **`property_url = listing.get('data-url')`**:\n",
    "        - Extracts the URL for the property details page.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Property Details Extraction**\n",
    "- If `title`, `address`, and `property_url` are valid:\n",
    "    - **`driver.get(property_url)`**:\n",
    "        - Navigates to the property details page.\n",
    "    - **`time.sleep(WAIT_TIME / 2)`**:\n",
    "        - Waits for the page to load.\n",
    "    - **`detail_soup = BeautifulSoup(driver.page_source, 'html.parser')`**:\n",
    "        - Parses the HTML content of the details page.\n",
    "    - **`unit_containers = detail_soup.find_all('li', class_='unitContainer js-unitContainerV3')`**:\n",
    "        - Finds all unit-specific details on the page.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Rental Type Identification**\n",
    "- **`rental_type = \"Unknown\"`**:\n",
    "    - Initializes the rental type as \"Unknown\".\n",
    "- **`og_title_tag = detail_soup.find(\"meta\", property=\"og:title\")`**:\n",
    "    - Extracts the `<meta>` tag containing the page title.\n",
    "- **`if og_title_tag and og_title_tag.get(\"content\"):`**:\n",
    "    - If the title exists, checks for keywords like \"house rental\", \"townhome\", \"condo\", or \"apartment\" to determine the rental type.\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Amenities Extraction**\n",
    "- **`amenities = extract_amenities(detail_soup)`**:\n",
    "    - Calls the `extract_amenities` function to extract amenities information from the details page.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Unit Details Extraction**\n",
    "- Iterates through each unit in `unit_containers`:\n",
    "    - Extracts details such as:\n",
    "        - **`unit_number`**: Unit number.\n",
    "        - **`price`**: Price of the unit.\n",
    "        - **`sqft`**: Square footage.\n",
    "        - **`beds`**: Number of bedrooms.\n",
    "        - **`baths`**: Number of bathrooms.\n",
    "    - Appends the extracted data to `all_units` as a dictionary, including:\n",
    "        - Property details, unit details, rental type, amenities, and the property URL.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9. Test Mode Limitation**\n",
    "- **`if TEST_MODE and len(all_units) >= MAX_UNITS:`**:\n",
    "    - If `TEST_MODE` is enabled and the number of scraped units reaches `MAX_UNITS`, the function stops scraping and returns the collected data as a Pandas DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "#### **10. Pagination**\n",
    "- **`if len(listings) < LISTINGS_PER_PAGE:`**:\n",
    "    - If the number of listings on the current page is less than the expected number (`LISTINGS_PER_PAGE`), it indicates the last page, and the loop breaks.\n",
    "- **`page += 1`**:\n",
    "    - Increments the page number to scrape the next page.\n",
    "\n",
    "---\n",
    "\n",
    "#### **11. Return Data**\n",
    "- **`return pd.DataFrame(all_units)`**:\n",
    "    - Converts the collected data into a Pandas DataFrame and returns it.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "This function automates the process of navigating through pages of property listings, extracting relevant details, and compiling them into a structured dataset for further analysis or storage.\n",
    "\n",
    "### Example Output\n",
    "The function returns a Pandas DataFrame with columns such as:\n",
    "- `Property`, `Address`, `Unit`, `Price`, `SqFt`, `Beds`, `Baths`, `RentalType`, `Phone`, `HasWasherDryer`, `HasAirConditioning`, `ListingURL`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc055e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df['Price'] = df['Price'].apply(extract_low_price)\n",
    "    df['SqFt'] = pd.to_numeric(df['SqFt'].astype(str).str.replace(',', '').str.extract(r'(\\d+)', expand=False), errors='coerce')\n",
    "    df['Beds'] = pd.to_numeric(df['Beds'], errors='coerce')\n",
    "    df['Baths'] = pd.to_numeric(df['Baths'], errors='coerce')\n",
    "\n",
    "    df['ZipCode'] = df['Address'].str.extract(r'(\\d{5})(?!.*\\d{5})')\n",
    "    city_state = df['Address'].str.extract(r',\\s*([^,]+),\\s*([A-Z]{2})\\s*\\d{5}')\n",
    "    df['City'] = city_state[0].str.strip()\n",
    "    df['State'] = city_state[1].str.strip()\n",
    "\n",
    "    df['PricePerSqFt'] = df.apply(\n",
    "        lambda row: round(row['Price'] / row['SqFt'], 2)\n",
    "        if pd.notnull(row['Price']) and pd.notnull(row['SqFt']) and row['SqFt'] > 0 else None,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    df['Beds_Baths'] = df.apply(\n",
    "        lambda row: f\"{int(row['Beds']) if pd.notnull(row['Beds']) else 'N/A'} Bed / {int(row['Baths']) if pd.notnull(row['Baths']) else 'N/A'} Bath\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    final_order = [\n",
    "        'Property', 'Address', 'City', 'State', 'ZipCode', 'Phone',\n",
    "        'Unit', 'Beds', 'Baths', 'Beds_Baths', 'SqFt', 'Price', 'PricePerSqFt',\n",
    "        'RentalType',\n",
    "        'HasWasherDryer', 'HasAirConditioning', 'HasPool', 'HasSpa',\n",
    "        'HasGym', 'HasEVCharging', 'StorageFee',\n",
    "        'AllowsDogs', 'AllowsCats',\n",
    "        'ListingURL'\n",
    "    ]\n",
    "    return df[final_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b5598",
   "metadata": {},
   "source": [
    "### Explanation of `clean_data` Function\n",
    "\n",
    "The `clean_data` function processes and cleans the raw data collected during web scraping to ensure it is structured, consistent, and ready for analysis or storage. Here's a detailed breakdown of its components:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Price Cleaning**\n",
    "```python\n",
    "df['Price'] = df['Price'].apply(extract_low_price)\n",
    "```\n",
    "- Applies the `extract_low_price` function to the `Price` column to extract the lowest price from various formats (e.g., \"$1,200-$1,500\" → `1200`).\n",
    "- Ensures the `Price` column contains numeric values or `None` for invalid entries.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Square Footage Cleaning**\n",
    "```python\n",
    "df['SqFt'] = pd.to_numeric(df['SqFt'].astype(str).str.replace(',', '').str.extract(r'(\\d+)', expand=False), errors='coerce')\n",
    "```\n",
    "- Converts the `SqFt` column to a string and removes commas (e.g., \"1,200\" → \"1200\").\n",
    "- Extracts numeric values using a regular expression (`\\d+`).\n",
    "- Converts the result to a numeric type using `pd.to_numeric`, coercing invalid entries to `NaN`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Beds and Baths Cleaning**\n",
    "```python\n",
    "df['Beds'] = pd.to_numeric(df['Beds'], errors='coerce')\n",
    "df['Baths'] = pd.to_numeric(df['Baths'], errors='coerce')\n",
    "```\n",
    "- Converts the `Beds` and `Baths` columns to numeric types, coercing invalid entries to `NaN`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Extracting Zip Code**\n",
    "```python\n",
    "df['ZipCode'] = df['Address'].str.extract(r'(\\d{5})(?!.*\\d{5})')\n",
    "```\n",
    "- Extracts the last 5-digit number from the `Address` column, which represents the ZIP code.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Extracting City and State**\n",
    "```python\n",
    "city_state = df['Address'].str.extract(r',\\s*([^,]+),\\s*([A-Z]{2})\\s*\\d{5}')\n",
    "df['City'] = city_state[0].str.strip()\n",
    "df['State'] = city_state[1].str.strip()\n",
    "```\n",
    "- Uses a regular expression to extract the city and state from the `Address` column.\n",
    "- The city is captured as the first group, and the state (2-letter abbreviation) is captured as the second group.\n",
    "- Strips any leading or trailing whitespace from the extracted values.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Calculating Price Per Square Foot**\n",
    "```python\n",
    "df['PricePerSqFt'] = df.apply(\n",
    "    lambda row: round(row['Price'] / row['SqFt'], 2)\n",
    "    if pd.notnull(row['Price']) and pd.notnull(row['SqFt']) and row['SqFt'] > 0 else None,\n",
    "    axis=1\n",
    ")\n",
    "```\n",
    "- Calculates the price per square foot by dividing the `Price` by `SqFt`.\n",
    "- Ensures both `Price` and `SqFt` are valid and `SqFt` is greater than 0 before performing the calculation.\n",
    "- Rounds the result to 2 decimal places or assigns `None` for invalid rows.\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Creating Beds and Baths Summary**\n",
    "```python\n",
    "df['Beds_Baths'] = df.apply(\n",
    "    lambda row: f\"{int(row['Beds']) if pd.notnull(row['Beds']) else 'N/A'} Bed / {int(row['Baths']) if pd.notnull(row['Baths']) else 'N/A'} Bath\",\n",
    "    axis=1\n",
    ")\n",
    "```\n",
    "- Creates a summary column combining the number of beds and baths in the format: \"X Bed / Y Bath\".\n",
    "- Replaces missing values with \"N/A\".\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Reordering Columns**\n",
    "```python\n",
    "final_order = [\n",
    "    'Property', 'Address', 'City', 'State', 'ZipCode', 'Phone',\n",
    "    'Unit', 'Beds', 'Baths', 'Beds_Baths', 'SqFt', 'Price', 'PricePerSqFt',\n",
    "    'RentalType',\n",
    "    'HasWasherDryer', 'HasAirConditioning', 'HasPool', 'HasSpa',\n",
    "    'HasGym', 'HasEVCharging', 'StorageFee',\n",
    "    'AllowsDogs', 'AllowsCats',\n",
    "    'ListingURL'\n",
    "]\n",
    "return df[final_order]\n",
    "```\n",
    "- Specifies the desired order of columns for the final DataFrame.\n",
    "- Returns the DataFrame with columns reordered according to `final_order`.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "The `clean_data` function ensures that the raw scraped data is cleaned, standardized, and organized into a consistent format. This makes the dataset ready for analysis, visualization, or storage in a CSV file.\n",
    "\n",
    "### Example Output\n",
    "After cleaning, the DataFrame will have columns such as:\n",
    "- `Property`, `Address`, `City`, `State`, `ZipCode`, `Phone`, `Unit`, `Beds`, `Baths`, `Beds_Baths`, `SqFt`, `Price`, `PricePerSqFt`, `RentalType`, `HasWasherDryer`, `HasAirConditioning`, `HasPool`, `HasSpa`, `HasGym`, `HasEVCharging`, `StorageFee`, `AllowsDogs`, `AllowsCats`, `ListingURL`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9487a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    driver = init_driver()\n",
    "    df = scrape_listings(driver)\n",
    "    driver.quit()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No data collected. File not saved.\")\n",
    "        logging.warning(\"No data collected. File not saved.\")\n",
    "    else:\n",
    "        df = clean_data(df)\n",
    "        filename = f'test_san_diego_county_rentals_{datetime.today().strftime(\"%Y-%m-%d\")}.csv'\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Scraping complete. Data saved to {filename}\")\n",
    "        logging.info(f\"Scraping complete. Data saved to {filename}\")\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    minutes, seconds = divmod(duration, 60)\n",
    "    print(f\"Script runtime: {int(minutes)} minutes and {seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42eea65",
   "metadata": {},
   "source": [
    "### Explanation of `main` Function\n",
    "\n",
    "The `main` function serves as the entry point for the web scraping script. It orchestrates the entire workflow, from initializing the web driver to saving the scraped data. Here's a detailed breakdown:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Start Timer**\n",
    "```python\n",
    "start_time = time.time()\n",
    "```\n",
    "- Records the start time of the script execution to calculate the total runtime later.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Initialize WebDriver**\n",
    "```python\n",
    "driver = init_driver()\n",
    "```\n",
    "- Calls the `init_driver` function to initialize a Selenium WebDriver with the specified configurations (e.g., headless mode).\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Scrape Listings**\n",
    "```python\n",
    "df = scrape_listings(driver)\n",
    "```\n",
    "- Calls the `scrape_listings` function to scrape property listings from the website.\n",
    "- The scraped data is returned as a Pandas DataFrame and stored in the variable `df`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Quit WebDriver**\n",
    "```python\n",
    "driver.quit()\n",
    "```\n",
    "- Closes the WebDriver to release system resources after scraping is complete.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Check for Empty Data**\n",
    "```python\n",
    "if df.empty:\n",
    "    print(\"No data collected. File not saved.\")\n",
    "    logging.warning(\"No data collected. File not saved.\")\n",
    "```\n",
    "- Checks if the DataFrame `df` is empty (i.e., no data was scraped).\n",
    "- If empty:\n",
    "  - Prints a message to the console.\n",
    "  - Logs a warning message to the log file.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Clean and Save Data**\n",
    "```python\n",
    "else:\n",
    "    df = clean_data(df)\n",
    "    filename = f'test_san_diego_county_rentals_{datetime.today().strftime(\"%Y-%m-%d\")}.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Scraping complete. Data saved to {filename}\")\n",
    "    logging.info(f\"Scraping complete. Data saved to {filename}\")\n",
    "```\n",
    "- If data was successfully scraped:\n",
    "  - Calls the `clean_data` function to clean and process the raw data.\n",
    "  - Generates a filename for the output CSV file using the current date (e.g., `test_san_diego_county_rentals_YYYY-MM-DD.csv`).\n",
    "  - Saves the cleaned DataFrame to a CSV file without including the index column.\n",
    "  - Prints a success message to the console and logs the same message.\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Calculate and Print Runtime**\n",
    "```python\n",
    "duration = time.time() - start_time\n",
    "minutes, seconds = divmod(duration, 60)\n",
    "print(f\"Script runtime: {int(minutes)} minutes and {seconds:.2f} seconds\")\n",
    "```\n",
    "- Calculates the total runtime of the script by subtracting the start time from the current time.\n",
    "- Converts the runtime into minutes and seconds using `divmod`.\n",
    "- Prints the runtime to the console in a human-readable format.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "The `main` function coordinates the entire scraping process, including:\n",
    "1. Initializing the WebDriver.\n",
    "2. Scraping property listings.\n",
    "3. Cleaning and saving the data.\n",
    "4. Logging warnings or success messages.\n",
    "5. Reporting the script's runtime.\n",
    "\n",
    "This function ensures the workflow is executed in a structured and efficient manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba4dce58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete. Data saved to test_san_diego_county_rentals_2025-04-25.csv\n",
      "Script runtime: 0 minutes and 16.16 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b94e30",
   "metadata": {},
   "source": [
    "### Explanation of `if __name__ == \"__main__\":`\n",
    "\n",
    "This construct is a common Python idiom used to ensure that certain code is executed only when the script is run directly, and not when it is imported as a module in another script. Here's a detailed explanation:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. `__name__` Variable**\n",
    "- In Python, every script has a special built-in variable called `__name__`.\n",
    "- When a script is run directly, `__name__` is set to `\"__main__\"`.\n",
    "- When a script is imported as a module in another script, `__name__` is set to the name of the script (e.g., `\"script_name\"`).\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Purpose of the Check**\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "```\n",
    "- This condition checks if the script is being run directly (i.e., `__name__` is `\"__main__\"`).\n",
    "- If `True`, the code block inside the `if` statement is executed.\n",
    "- If the script is imported as a module, the code block is skipped.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Calling the `main` Function**\n",
    "```python\n",
    "main()\n",
    "```\n",
    "- If the script is run directly, the `main()` function is called to execute the workflow defined in the script.\n",
    "- This ensures that the script's functionality is triggered only when intended.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Why It's Useful**\n",
    "- **Prevents Unintended Execution**: Ensures that the script's main logic doesn't run automatically when the script is imported as a module.\n",
    "- **Modularity**: Allows the script to be reused as a module in other scripts without executing its main logic.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "```python\n",
    "# script.py\n",
    "def main():\n",
    "    print(\"This is the main function.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "- **When run directly**:\n",
    "```bash\n",
    "$ python script.py\n",
    "This is the main function.\n",
    "```\n",
    "\n",
    "- **When imported**:\n",
    "```python\n",
    "# another_script.py\n",
    "import script\n",
    "# No output, as `main()` is not called automatically.\n",
    "```\n",
    "\n",
    "This construct is essential for writing reusable and modular Python code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
