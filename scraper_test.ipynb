{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f375789",
   "metadata": {},
   "source": [
    "# Web Scraper Test Script\n",
    "\n",
    "## Designed to collect rental property data **Apartments.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5de133",
   "metadata": {},
   "source": [
    "This script is a web scraper designed to collect rental property data from the website **apartments.com** for properties in San Diego County, California, under $4,000. It uses **Selenium** for browser automation and **BeautifulSoup** for HTML parsing. The script processes the data, cleans it, and saves it to a CSV file. Here's a breakdown of its functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aab9b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3f96b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADLESS = True\n",
    "WAIT_TIME = 4\n",
    "LISTINGS_PER_PAGE = 40\n",
    "BASE_URL = \"https://www.apartments.com/apartments-condos/san-diego-county-ca/under-4000/\"\n",
    "LOG_FILE = \"scraper_log.txt\"\n",
    "TEST_MODE = True\n",
    "MAX_UNITS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2161d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    filemode='w',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.WARNING\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc77cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_driver():\n",
    "    options = Options()\n",
    "    if HEADLESS:\n",
    "        options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "    service = Service(EdgeChromiumDriverManager().install(), log_output=os.devnull)\n",
    "    return webdriver.Edge(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c6c8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_low_price(price):\n",
    "    if pd.isna(price):\n",
    "        return None\n",
    "    price = re.sub(r'[^\\d\\-]', '', str(price))\n",
    "    return float(price.split('-')[0]) if '-' in price else float(price) if price.isdigit() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01484a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_amenities(soup):\n",
    "    labels = soup.select('.amenityLabel') + soup.select('.combinedAmenitiesList li span')\n",
    "    text = ' '.join(el.get_text(separator=' ').lower().strip() for el in labels)\n",
    "\n",
    "    logging.debug(\"Combined amenities text: %s\", text)\n",
    "\n",
    "    return {\n",
    "        'HasWasherDryer': 'washer/dryer' in text or 'in unit washer' in text,\n",
    "        'HasAirConditioning': 'air conditioning' in text,\n",
    "        'HasPool': 'pool' in text,\n",
    "        'HasSpa': 'spa' in text or 'hot tub' in text,\n",
    "        'HasGym': 'fitness center' in text or 'gym' in text,\n",
    "        'HasEVCharging': 'ev charging' in text,\n",
    "        'AllowsDogs': 'dogs allowed' in text or 'dog friendly' in text,\n",
    "        'AllowsCats': 'cats allowed' in text or 'cat friendly' in text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cfa8328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_listings(driver):\n",
    "    all_units = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        url = f\"{BASE_URL}{page}/\"\n",
    "        logging.info(f\"Scraping page {page}: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(WAIT_TIME)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        listings = soup.find_all('article')\n",
    "\n",
    "        if not listings:\n",
    "            break\n",
    "\n",
    "        for listing in listings:\n",
    "            title = listing.find('span', class_='js-placardTitle')\n",
    "            address = listing.find('div', class_='property-address')\n",
    "            phone = listing.find('button', class_='phone-link')\n",
    "            property_url = listing.get('data-url')\n",
    "\n",
    "            if title and address and property_url:\n",
    "                try:\n",
    "                    driver.get(property_url)\n",
    "                    time.sleep(WAIT_TIME / 2)\n",
    "                    detail_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    unit_containers = detail_soup.find_all('li', class_='unitContainer js-unitContainerV3')\n",
    "\n",
    "                    rental_type = \"Unknown\"\n",
    "                    og_title_tag = detail_soup.find(\"meta\", property=\"og:title\")\n",
    "                    if og_title_tag and og_title_tag.get(\"content\"):\n",
    "                        content = og_title_tag[\"content\"].lower()\n",
    "                        for term in [\"house rental\", \"townhome\", \"condo\", \"apartment\"]:\n",
    "                            if term in content:\n",
    "                                rental_type = term.replace(\" rental\", \"\").capitalize()\n",
    "\n",
    "                    amenities = extract_amenities(detail_soup)\n",
    "\n",
    "                    for unit in unit_containers:\n",
    "                        unit_number = unit.find('div', class_='unitColumn column')\n",
    "                        price = unit.find('div', class_='pricingColumn column')\n",
    "                        sqft = unit.find('div', class_='sqftColumn column')\n",
    "                        beds = unit.get('data-beds')\n",
    "                        baths = unit.get('data-baths')\n",
    "\n",
    "                        all_units.append({\n",
    "                            'Property': title.text.strip(),\n",
    "                            'Address': address.text.strip(),\n",
    "                            'Unit': unit_number.text.strip() if unit_number else \"N/A\",\n",
    "                            'Price': price.text.strip() if price else \"N/A\",\n",
    "                            'SqFt': sqft.text.strip() if sqft else \"N/A\",\n",
    "                            'Beds': beds if beds else \"N/A\",\n",
    "                            'Baths': baths if baths else \"N/A\",\n",
    "                            'RentalType': rental_type,\n",
    "                            'Phone': phone.get('phone-data') if phone and phone.has_attr('phone-data') else \"N/A\",\n",
    "                            **amenities,\n",
    "                            'StorageFee': None,\n",
    "                            'ListingURL': property_url\n",
    "                        })\n",
    "\n",
    "                        if TEST_MODE and len(all_units) >= MAX_UNITS:\n",
    "                            logging.info(f\"TEST_MODE: Stopping after {MAX_UNITS} listings.\")\n",
    "                            return pd.DataFrame(all_units)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error processing {property_url}: {e}\")\n",
    "\n",
    "        if len(listings) < LISTINGS_PER_PAGE:\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "    return pd.DataFrame(all_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc055e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df['Price'] = df['Price'].apply(extract_low_price)\n",
    "    df['SqFt'] = pd.to_numeric(df['SqFt'].astype(str).str.replace(',', '').str.extract(r'(\\d+)', expand=False), errors='coerce')\n",
    "    df['Beds'] = pd.to_numeric(df['Beds'], errors='coerce')\n",
    "    df['Baths'] = pd.to_numeric(df['Baths'], errors='coerce')\n",
    "\n",
    "    df['ZipCode'] = df['Address'].str.extract(r'(\\d{5})(?!.*\\d{5})')\n",
    "    city_state = df['Address'].str.extract(r',\\s*([^,]+),\\s*([A-Z]{2})\\s*\\d{5}')\n",
    "    df['City'] = city_state[0].str.strip()\n",
    "    df['State'] = city_state[1].str.strip()\n",
    "\n",
    "    df['PricePerSqFt'] = df.apply(\n",
    "        lambda row: round(row['Price'] / row['SqFt'], 2)\n",
    "        if pd.notnull(row['Price']) and pd.notnull(row['SqFt']) and row['SqFt'] > 0 else None,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    df['Beds_Baths'] = df.apply(\n",
    "        lambda row: f\"{int(row['Beds']) if pd.notnull(row['Beds']) else 'N/A'} Bed / {int(row['Baths']) if pd.notnull(row['Baths']) else 'N/A'} Bath\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    final_order = [\n",
    "        'Property', 'Address', 'City', 'State', 'ZipCode', 'Phone',\n",
    "        'Unit', 'Beds', 'Baths', 'Beds_Baths', 'SqFt', 'Price', 'PricePerSqFt',\n",
    "        'RentalType',\n",
    "        'HasWasherDryer', 'HasAirConditioning', 'HasPool', 'HasSpa',\n",
    "        'HasGym', 'HasEVCharging', 'StorageFee',\n",
    "        'AllowsDogs', 'AllowsCats',\n",
    "        'ListingURL'\n",
    "    ]\n",
    "    return df[final_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9487a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    driver = init_driver()\n",
    "    df = scrape_listings(driver)\n",
    "    driver.quit()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No data collected. File not saved.\")\n",
    "        logging.warning(\"No data collected. File not saved.\")\n",
    "    else:\n",
    "        df = clean_data(df)\n",
    "        filename = f'test_san_diego_county_rentals_{datetime.today().strftime(\"%Y-%m-%d\")}.csv'\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Scraping complete. Data saved to {filename}\")\n",
    "        logging.info(f\"Scraping complete. Data saved to {filename}\")\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    minutes, seconds = divmod(duration, 60)\n",
    "    print(f\"Script runtime: {int(minutes)} minutes and {seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba4dce58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete. Data saved to test_san_diego_county_rentals_2025-04-25.csv\n",
      "Script runtime: 0 minutes and 16.16 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd5342d",
   "metadata": {},
   "source": [
    "This script is a web scraper designed to collect rental property data from the website **apartments.com** for properties in San Diego County, California, under $4,000. It uses **Selenium** for browser automation and **BeautifulSoup** for HTML parsing. The script processes the data, cleans it, and saves it to a CSV file. Here's a breakdown of its functionality:\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Components**\n",
    "\n",
    "1. **Imports and Configuration**\n",
    "   - Libraries like `selenium`, `BeautifulSoup`, `pandas`, `logging`, and `re` are imported.\n",
    "   - Constants are defined:\n",
    "     - `HEADLESS`: Runs the browser in headless mode (no GUI).\n",
    "     - `WAIT_TIME`: Time to wait for pages to load.\n",
    "     - `LISTINGS_PER_PAGE`: Number of listings per page.\n",
    "     - `BASE_URL`: The URL for the search query.\n",
    "     - `LOG_FILE`: File for logging warnings and errors.\n",
    "     - `TEST_MODE` and `MAX_UNITS`: Used for testing to limit the number of listings scraped.\n",
    "\n",
    "2. **`init_driver()`**\n",
    "   - Initializes a Selenium WebDriver for Microsoft Edge.\n",
    "   - Configures the browser to run in headless mode and sets a custom user-agent.\n",
    "\n",
    "3. **`extract_low_price(price)`**\n",
    "   - Extracts the lowest price from a price string (e.g., \"$1,200-$1,500\" â†’ `1200`).\n",
    "\n",
    "4. **`extract_amenities(soup)`**\n",
    "   - Parses the HTML of a property page to extract amenities like washer/dryer, air conditioning, pool, gym, etc.\n",
    "   - Returns a dictionary of boolean values indicating the presence of each amenity.\n",
    "\n",
    "5. **`scrape_listings(driver)`**\n",
    "   - The main scraping function:\n",
    "     - Iterates through pages of listings.\n",
    "     - For each listing, navigates to the property detail page and extracts data such as:\n",
    "       - Property name, address, unit details, price, square footage, beds, baths, rental type, phone number, and amenities.\n",
    "     - Appends the data to a list of dictionaries (`all_units`).\n",
    "     - Stops scraping if `TEST_MODE` is enabled and the maximum number of units is reached.\n",
    "   - Returns a Pandas DataFrame containing all the scraped data.\n",
    "\n",
    "6. **`clean_data(df)`**\n",
    "   - Cleans and processes the scraped data:\n",
    "     - Converts price, square footage, beds, and baths to numeric values.\n",
    "     - Extracts city, state, and ZIP code from the address.\n",
    "     - Calculates price per square foot.\n",
    "     - Creates a \"Beds_Baths\" column summarizing the number of beds and baths.\n",
    "     - Reorders columns into a final format for saving.\n",
    "\n",
    "7. **`main()`**\n",
    "   - The entry point of the script:\n",
    "     - Initializes the WebDriver.\n",
    "     - Calls `scrape_listings()` to collect data.\n",
    "     - Cleans the data using `clean_data()`.\n",
    "     - Saves the cleaned data to a CSV file named with the current date.\n",
    "     - Logs the runtime of the script.\n",
    "\n",
    "---\n",
    "\n",
    "### **Workflow**\n",
    "\n",
    "1. **Initialization**\n",
    "   - The script sets up the WebDriver and logging.\n",
    "\n",
    "2. **Scraping**\n",
    "   - It navigates through pages of listings on **apartments.com**.\n",
    "   - For each listing, it extracts relevant details and amenities.\n",
    "\n",
    "3. **Data Cleaning**\n",
    "   - The raw data is processed to ensure consistency and usability.\n",
    "\n",
    "4. **Saving Results**\n",
    "   - The cleaned data is saved to a CSV file in the format: `san_diego_county_rentals_YYYY-MM-DD.csv`.\n",
    "\n",
    "5. **Runtime Logging**\n",
    "   - The script logs warnings, errors, and runtime information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Features**\n",
    "- **Headless Mode**: Allows the script to run without opening a browser window.\n",
    "- **Error Handling**: Logs warnings for errors encountered during scraping.\n",
    "- **Test Mode**: Limits the number of listings scraped for testing purposes.\n",
    "- **Data Cleaning**: Ensures the final dataset is well-structured and ready for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### **Output**\n",
    "The script generates a CSV file containing the following columns:\n",
    "- Property details (name, address, city, state, ZIP code, phone number, etc.).\n",
    "- Unit details (price, square footage, beds, baths, rental type).\n",
    "- Amenities (washer/dryer, air conditioning, pool, gym, etc.).\n",
    "- Calculated fields (price per square foot, beds/baths summary).\n",
    "\n",
    "---\n",
    "\n",
    "### **Usage**\n",
    "Run the script by executing it in the terminal:\n",
    "```bash\n",
    "python scraper.py\n",
    "```\n",
    "Ensure that the required dependencies (e.g., Selenium, BeautifulSoup, Pandas) are installed, and the WebDriver for Edge is properly configured."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
